save_model: /data/wangke/wmt20/save/torch_model/mlbert/model

data: /data/wangke/wmt20/data/torch_data/train/data
train_steps: 50000
# start_decay_steps: 5000
valid_steps: 500
save_checkpoint_steps: 500
keep_checkpoint: 200

# Dimensionality
rnn_size: 768 #!
word_vec_size: 768 #!
transformer_ff: 3072 #!
heads: 12 #!
layers: 12 #!

# Embeddings
position_encoding: 'true' #!
share_embeddings: 'true' #!
share_decoder_embeddings: 'true' #!

# Encoder
encoder_type: bert #!
enc_bert_type: bert-base-multilingual-cased #!

# Decoder
decoder_type: bert #!
dec_bert_type: bert-base-multilingual-cased #!
bert_decoder_token_type: B #!

# Layer Sharing
bert_decoder_init_context: 'true'
share_self_attn: 'true'
# tie_context_attn: 'true'
# share_feed_forward: 'true'

# Regularization
dropout: 0.1
label_smoothing: 0.1

# Optimization
optim: bertadam #!
learning_rate: 0.00005
warmup_steps: 5000
batch_type: tokens
normalization: tokens
accum_count: 4
batch_size: 1024
max_grad_norm: 0
param_init: 0
param_init_glorot: 'true'
valid_batch_size: 64

average_decay: 0.0001

# GPU
seed: 42
world_size: 8
gpu_ranks:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
